{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = DatabricksSession.builder.profile(\"marvelmlops\").getOrCreate()\n",
    "\n",
    "df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to set up automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.config import ProjectConfig\n",
    "import numpy as np\n",
    "from packages.paths import AllPaths\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "ALL_PATHS = AllPaths()\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=ALL_PATHS.filename_config)\n",
    "\n",
    "df = spark.read.option(\"header\",True).csv(f'{ALL_PATHS.data_volume}/hotel_reservations.csv').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Function to get the current git SHA\n",
    "def get_git_sha():\n",
    "    try:\n",
    "        git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()\n",
    "    except Exception as e:\n",
    "        git_sha = \"unknown\"\n",
    "    return git_sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri('databricks-uc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook sourc\n",
    "\n",
    "# Extract configuration details\n",
    "num_features = config.num_features\n",
    "cat_features = config.cat_features\n",
    "target = config.target\n",
    "parameters = config.parameters\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing sets from Databricks tables\n",
    "train_set_spark = spark.table(f\"{catalog_name}.{schema_name}.train_set\")\n",
    "train_set = spark.table(f\"{catalog_name}.{schema_name}.train_set\").toPandas()\n",
    "test_set = spark.table(f\"{catalog_name}.{schema_name}.test_set\").toPandas()\n",
    "\n",
    "X_train = train_set[num_features + cat_features]\n",
    "y_train = train_set[target]\n",
    "\n",
    "X_test = test_set[num_features + cat_features]\n",
    "y_test = test_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# Define the preprocessor for categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and the LightGBM regressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LGBMClassifier(**parameters))\n",
    "])\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "mlflow.set_experiment(experiment_names=[\"/Shared/hotel-reservations-cremerf\"])\n",
    "git_sha = \"fb7d5ec632172615cb88ee17ceef29ee57702a73\"\n",
    "\n",
    "# Start an MLflow run to track the training process\n",
    "with mlflow.start_run(\n",
    "    tags={\"git_sha\": f\"{git_sha}\",\n",
    "          \"branch\": \"week2\"},\n",
    ") as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "\n",
    "    # Log parameters, metrics, and the model to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(parameters)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    signature = infer_signature(model_input=X_train, model_output=y_pred)\n",
    "\n",
    "    dataset = mlflow.data.from_spark(\n",
    "    train_set_spark, table_name=f\"{catalog_name}.{schema_name}.train_set\",\n",
    "    version=\"0\")\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"lightgbm-pipeline-model\",\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f'runs:/{run_id}/lightgbm-pipeline-model',\n",
    "    name=f\"{catalog_name}.{schema_name}.house_prices_model_basic\",\n",
    "    tags={\"git_sha\": f\"{git_sha}\"})\n",
    "\n",
    "# COMMAND ----------\n",
    "run = mlflow.get_run(run_id)\n",
    "dataset_info = run.inputs.dataset_inputs[0].dataset\n",
    "dataset_source = mlflow.data.get_source(dataset_info)\n",
    "dataset_source.load()\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Instantiate LGBMClassifier with parameters\n",
    "model = lgb.LGBMClassifier(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_sha = get_git_sha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fb7d5ec632172615cb88ee17ceef29ee57702a73'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = config.num_features\n",
    "for col in num_features:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = config.cat_features\n",
    "for cat_col in cat_features:\n",
    "    df[cat_col] = df[cat_col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['avg_price_per_room'], bins=50)\n",
    "plt.title('Distribution of Average Price per Room')\n",
    "plt.xlabel('Average Price per Room')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude zeros from calculations\n",
    "non_zero_prices = df.loc[df['avg_price_per_room'] != 0, 'avg_price_per_room']\n",
    "\n",
    "mean_price = non_zero_prices.mean()\n",
    "median_price = non_zero_prices.median()\n",
    "\n",
    "print(f\"Mean Price: {mean_price}\")\n",
    "print(f\"Median Price: {median_price}\")\n",
    "\n",
    "for col in cat_features:\n",
    "    # Ensure the column is of type 'category'\n",
    "    if not isinstance(df[col].dtype, CategoricalDtype):\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Add 'Unknown' to categories if not already present\n",
    "    if 'Unknown' not in df[col].cat.categories:\n",
    "        df[col] = df[col].cat.add_categories(['Unknown'])\n",
    "    \n",
    "    # Fill NaN values with 'Unknown'\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "df[config.target] = df[config.target].map({'Not_Canceled': 0, 'Canceled': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply median imputation to null or zero values in avg_price_per_room to avoid right skewness of the data\n",
    "# check what kind of preprocessing could we perform to categorical data\n",
    "# remember to put scalling and encoding to the modelling module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['booking_status'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['booking_status'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "class_percentages = df['booking_status'].value_counts(normalize=True) * 100\n",
    "print(class_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['booking_status_encoded'] = le.fit_transform(df['booking_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from mlflow.models import infer_signature\n",
    "from packages.config import ProjectConfig\n",
    "import json\n",
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "ALLPATHS = AllPaths()\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=ALLPATHS.filename_config)\n",
    "logger.info(\"Configuration loaded:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_tracking_uri(\"databricks://marvelmlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tracking_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = mlflow.search_runs(\n",
    "    experiment_names=[\"/Shared/cremerf-cancellation-prediction\"],\n",
    "    filter_string=\"tags.branch='week2'\",\n",
    ").run_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['booking_status_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_target = df['booking_status_encoded'].isnull().sum()\n",
    "print(f\"Missing values in target variable: {missing_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "# Count zeros in numeric columns\n",
    "zero_counts = (df[numeric_cols] == 0).sum()\n",
    "\n",
    "# Count NaN/null values in numeric columns\n",
    "nan_counts = df[numeric_cols].isna().sum()\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Zero Count': zero_counts,\n",
    "    'NaN/Null Count': nan_counts\n",
    "})\n",
    "\n",
    "# Display the summary\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric and categorical features\n",
    "numeric_features = [\n",
    "    'no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights',\n",
    "    'lead_time', 'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled',\n",
    "    'avg_price_per_room', 'no_of_special_requests'\n",
    "]\n",
    "categorical_features = [\n",
    "    'type_of_meal_plan', 'required_car_parking_space', 'room_type_reserved', \n",
    "    'arrival_year', 'arrival_month', 'arrival_date', 'market_segment_type', \n",
    "    'repeated_guest'\n",
    "]\n",
    "\n",
    "# Preprocessing for numeric data: convert data types and scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())  # Scale data\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data: fill missing values and apply one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Convert categorical data\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # This will drop other columns not listed explicitly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = df.drop('booking_status', axis=1)\n",
    "y = df['booking_status']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True).csv('/Volumes/mlops_students/cremerfederico29/data/hotel_reservations.csv').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
